{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf07c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = [\n",
    "    '_C3_81frica.txt','Aeropuerto.txt','Agricultura.txt','Alan_Turing.txt',\n",
    "    'Antiguo_Egipto.txt','Arte.txt','Asia.txt','Babilonia_(ciudad).txt',\n",
    "    'Biblia.txt','Biodiversidad.txt','Capitalismo.txt','Ciencia.txt','Colombia.txt',\n",
    "    'Consumo.txt','Cristianismo.txt','Cristo.txt','Cultura.txt','Democracia.txt',\n",
    "    'Diccionario.txt','Divina_comedia.txt','Ecologismo.txt','Energ_C3_ADa_nuclear.txt',\n",
    "    'Enrique_VIII_de_Inglaterra.txt','Escritura.txt','Est_C3_A9tica.txt','Estado.txt',\n",
    "    'Estados_Unidos.txt','Europa.txt','Filosof_C3_ADa.txt','Finanzas.txt','Fruta.txt',\n",
    "    'Gabriel_Garc_C3_ADa_M_C3_A1rquez.txt','Gastronom_C3_ADa_de_M_C3_A9xico.txt','George_Patton.txt',\n",
    "    'Grecia.txt','Guerra_Fr_C3_ADa.txt','Henry_Ford.txt','Holocausto.txt','Homo_sapiens.txt',\n",
    "    'Inteligencia_artificial.txt','Italia.txt','Jorge_Isaacs.txt','Juda_C3_ADsmo.txt','Juglar.txt',\n",
    "    'Julio_Garavito.txt','Liberalismo.txt','Libro.txt','Ludwig_van_Beethoven.txt','Nevado_del_Ruiz.txt',\n",
    "    'Nikola_Tesla.txt','Odisea.txt','Organizaci_C3_B3n_de_las_Naciones_Unidas.txt',\n",
    "    'Per_C3_ADodo_helen_C3_ADstico.txt','Pizza.txt','Pueblos_germ_C3_A1nicos.txt','Real_Academia_Espa_C3_B1ola.txt',\n",
    "    'Reino_Unido.txt','Revoluci_C3_B3n_Industrial.txt','Rueda.txt','Salsa_(g_C3_A9nero_musical).txt',\n",
    "    'Saxof_C3_B3n.txt','Segunda_Guerra_Mundial.txt','Sublime.txt','Thomas_Alva_Edison.txt','Valle.txt'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3962db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_test = open('Estados_Unidos2.txt','r',encoding='utf-8')\n",
    "texto = file_test.read()\n",
    "#print(texto)\n",
    "file_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f05ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('Estados_Unidos2.txt', 'r',encoding='utf-8') as raw_file:\n",
    "\n",
    "  logs_text = raw_file.read()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ab5656d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pingüino: Malñãgà ês uñ̺ã cieeauèdãd fantañstica y èn Logroño me pica el... moñǫ̝̘̦̞̟̩̐̏̋͌ͬ̚͡õ̪͓͍̦̓ơ̤̺̬̯͂̌͐͐͟o͎͈̳̠̼̫͂̊\n",
      "¡Hola, Mundo!\n",
      "¡HOLA, MUNDO!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from unicodedata import normalize\n",
    "\n",
    "s = \"Pingüino: Málñãgà ês uñ̺ã cíééáuèdãd fantáñstica y èn Logroño me pica el... moñǫ̝̘̦̞̟̩̐̏̋͌́ͬ̚͡õ̪͓͍̦̓ơ̤̺̬̯͂̌͐͐͟o͎͈̳̠̼̫͂̊\"\n",
    "\n",
    "# -> NFD y eliminar diacríticos\n",
    "s = re.sub(\n",
    "        r\"([^\\u0301])[\\u0301]\", r\"\\1\", \n",
    "        #r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n",
    "        normalize( \"NFD\", s)\n",
    "    )\n",
    "\n",
    "# -> NFC\n",
    "s = normalize( 'NFC', s)\n",
    "\n",
    "print( s )\n",
    "\n",
    "s = \"Pingüino: Málñãgà ês uñ̺ã cíééáuèdãd fantáñstica y èn Logroño me pica el... moñǫ̝̘̦̞̟̩̐̏̋͌́ͬ̚͡õ̪͓͍̦̓ơ̤̺̬̯͂̌͐͐͟o͎͈̳̠̼̫͂̊\"\n",
    "\n",
    "\n",
    "def normalize(s):\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "    )\n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "    return s\n",
    "print(normalize(\"¡Hólá, Múndó!\"))\n",
    "print(normalize(\"¡HÓLÁ, MÚNDÓ!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c72921",
   "metadata": {},
   "source": [
    "unicodedata.normalize(form, unistr)\n",
    "Return the normal form form for the Unicode string unistr. Valid values for form are ‘NFC’, ‘NFKC’, ‘NFD’, and ‘NFKD’.\n",
    "\n",
    "The Unicode standard defines various normalization forms of a Unicode string, based on the definition of canonical equivalence and compatibility equivalence. In Unicode, several characters can be expressed in various way. For example, the character U+00C7 (LATIN CAPITAL LETTER C WITH CEDILLA) can also be expressed as the sequence U+0043 (LATIN CAPITAL LETTER C) U+0327 (COMBINING CEDILLA).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e1ac6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6c2d40f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "pingüino αποκεντρωμένες  Hola3muñndo Estados Unidos[nota 2]​ (EE. UU.; en inglés, United States \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pingüino αποκεντρωμένες  Hola3muñndo Estados Unidos[nota 2]​ (EE. UU.; en ingles, United States \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pingüino αποκεντρωμένες  Hola3muñndo Estados Unidos nota 2    EE  UU   en ingles  United States \n",
      "    ping ino                 Hola muñndo Estados Unidos nota      EE  UU   en ingles  United States \n",
      "    ping ino                 Hola muñndo Estados Unidos nota           en ingles  United States o   \n",
      "    ping ino                 hola muñndo estados unidos nota           en ingles  united states o   \n"
     ]
    }
   ],
   "source": [
    "# ¿Cómo puedo reemplazar las letras con tildes por las mismas sin tilde, pero no la “ñ”?\n",
    "#  https://es.stackoverflow.com/q/135707/127\n",
    "\n",
    "def doble_upper(match_obj):\n",
    "    char_elem = match_obj.group(0)\n",
    "    count = 0\n",
    "    for letter in char_elem:\n",
    "        if letter.isupper():\n",
    "            count +=1\n",
    "        if count >= 2:\n",
    "            return \"\"\n",
    "            break\n",
    "    return char_elem\n",
    "print(logs_text[:100])\n",
    "from unicodedata import normalize\n",
    "# -> NFD y eliminar diacríticos\n",
    "#remove_diacriticals = re.sub(\n",
    "#        r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n",
    "#        normalize( \"NFD\", logs_text), 0, re.I\n",
    "    \n",
    "#remove_diacriticals = re.sub(r\"([^\\u0301])[\\u0301]\",r\"\\1\", \n",
    "#        normalize( \"NFD\", logs_text), 0, re.I\n",
    "#        )\n",
    "#remove_diacriticals = normalize( 'NFC', remove_diacriticals)\n",
    "def normalize(s):\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "    )\n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "    return s\n",
    "remove_diacriticals = normalize(logs_text)\n",
    "print(remove_diacriticals[:100])\n",
    "##\n",
    "remove_symbols = re.sub(r'[^\\w^n\\u0303|\\s]|_', ' ', remove_diacriticals)\n",
    "print(remove_symbols[:100])\n",
    "##\n",
    "only_letters = re.sub(r\"[^a-zñA-ZÑ]\", \" \", remove_symbols)\n",
    "#only_letters = re.sub(r\"\\S*[^a-zñA-ZÑ\\s]\\S*\", \"\", remove_symbols)\n",
    "print(only_letters[:100])\n",
    "##\n",
    "\n",
    "remove_acronyms = re.sub(r\"\\S*\\w\\S*\", doble_upper, only_letters)\n",
    "print(remove_acronyms[:100])\n",
    "##----------------\n",
    "def reverse_case(match_obj):\n",
    "    char_elem = match_obj.group(0)\n",
    "    return char_elem.lower()\n",
    "\n",
    "remove_uppercase = re.sub(r\"([A-ZÑ])\",reverse_case, remove_acronyms)\n",
    "print(remove_uppercase[:100] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02facb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7185\n"
     ]
    }
   ],
   "source": [
    "list_of_words = re.sub(\"[^\\w|^n\\u0303]\", \" \",remove_uppercase).split()\n",
    "#print(list_of_words[:100])\n",
    "list_large_words=[]\n",
    "for word in list_of_words:\n",
    "    #print(word)\n",
    "    #print(len('año'))\n",
    "    if len(word)>3:\n",
    "        #print(len(word))\n",
    "        list_large_words.append(word)\n",
    "print(len(list_large_words))\n",
    "#print(list_large_words[:100])\n",
    "#print(len('año'))\n",
    "# -> NFC\n",
    "#logs_text = normalize( 'NFC', logs_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print(\"\\n-------------------------------------------------------------------\\n\")\n",
    "stop_words = stopwords.words('spanish')\n",
    "list_filter_words = []\n",
    "\n",
    "for word in list_large_words:\n",
    "    if word not in stop_words:\n",
    "        list_filter_words.append(word)\n",
    "print(list_filter_words[:100])\n",
    "print(len(list_filter_words))\n",
    "##\n",
    "data_base_filter = open(\"Data_Base_Words_Filter.txt\", \"a\")\n",
    "\n",
    "\n",
    "list_final_words = []\n",
    "for word in list_filter_words:\n",
    "    if word not in list_final_words:\n",
    "        list_final_words.append(word)\n",
    "##\n",
    "for word in list_filter_words:\n",
    "    data_base_filter.write(word)\n",
    "    data_base_filter.write(' ')\n",
    "data_base_filter.close()  \n",
    "##\n",
    "data_base_final = open(\"Data_Base_Final_Words.txt\", \"a\")\n",
    "\n",
    "print(list_final_words[:100])\n",
    "print(len(list_final_words))\n",
    "\n",
    "for word in list_final_words:\n",
    "    data_base_final.write(word)\n",
    "    data_base_final.write(' ')\n",
    "data_base_final.close()       \n",
    "print('estados' in list_filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data_Base_Words_Filter.txt\", 'r') as raw_file:\n",
    "\n",
    "  logs_text = raw_file.read()\n",
    "\n",
    "#print(logs_text)\n",
    "words_filter_list = re.sub(\"[\\s]\", \" \",logs_text).split()\n",
    "print(len(words_filter_list))\n",
    "\n",
    "with open(\"Data_Base_Final_Words.txt\", 'r') as raw_file:\n",
    "\n",
    "  logs_text = raw_file.read()\n",
    "\n",
    "#print(logs_text)\n",
    "words_final_list = re.sub(\"[\\s]\", \" \",logs_text).split()\n",
    "print(len(words_final_list))\n",
    "print (max(words_final_list,key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bf066",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "            'n','ñ', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "#print(words_final_list)\n",
    "def position_dict(letters_list,words_list):\n",
    "    dicts = {}\n",
    "    for letter in letters_list:\n",
    "        for word in words_list:\n",
    "            #print(word)\n",
    "            for letter1 in word:\n",
    "                if letter == letter1:\n",
    "                    if letter not in dicts:\n",
    "                        dicts[letter] = []\n",
    "                    dicts[letter].append(word.index(letter1)+1)\n",
    "    return dicts\n",
    "word = words_final_list[1]\n",
    "position_letters_dict = position_dict(alphabet,words_final_list)\n",
    "\n",
    "\n",
    "print(len(position_letters_dict['a']))\n",
    "print(len(position_letters_dict))\n",
    "print(len(position_letters_dict['b']))\n",
    "list_letter_repetitions=[]\n",
    "for j in position_letters_dict:\n",
    "    list_letter_repetitions.append(len(position_letters_dict[j]))\n",
    "\n",
    "print(list_letter_repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as st\n",
    "def list_dict_data(parameter,dictionary):\n",
    "    b= (max(dictionary[parameter]))\n",
    "    list_dict=(dictionary[parameter])\n",
    "    position_sum=[0 for i in range(b+1)]\n",
    "    #print(list_dict)\n",
    "    for j in list_dict:\n",
    "        #print(j)\n",
    "        position_sum[j] += 1\n",
    "    del position_sum[0]\n",
    "    Q1 = np.percentile(dictionary[parameter], 25)\n",
    "    Q2 = np.percentile(dictionary[parameter], 50)\n",
    "    Q3 = np.percentile(dictionary[parameter], 75)\n",
    "    P99 = np.percentile(dictionary[parameter], 99)\n",
    "    median = np.median(dictionary[parameter])\n",
    "    mode = st.mode(dictionary[parameter])\n",
    "    mean = np.mean(dictionary[parameter])\n",
    "    return [position_sum,Q1,Q2,Q3,median,mode,mean,P99,parameter]\n",
    "positions= list_dict_data('e',position_letters_dict)\n",
    "print(positions[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09364484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# create a dataset\n",
    "height = positions[0]\n",
    "bars = [i+1 for i in range(len(positions[0]))]\n",
    "x_pos = np.arange(len(bars))\n",
    "\n",
    "# Create bars\n",
    "plt.bar(x_pos, height, color=(0.2, 0.4, 0.6, 0.6))\n",
    "\n",
    "# Create names on the x-axis\n",
    "plt.xticks(x_pos, bars)\n",
    "plt.axvline(positions[1]-1,color = 'b',label = 'Q1')\n",
    "plt.axvline(positions[2]-1,color = 'g',label = 'Q2')\n",
    "plt.axvline(positions[3]-1,color = 'y',label = 'Q3')\n",
    "plt.axvline(positions[7]-1,color = 'r',label = 'P99')\n",
    "# Show graph\n",
    "plt.title('Diagrama de barras - posición letra \"{}\"'.format('a'))\n",
    "plt.xlabel('Posicion')\n",
    "plt.ylabel('Repeticiones')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('La mediana de la posicion es: {}'.format(positions[4]))\n",
    "print('La moda de la posicion es: {}'.format(positions[5]))\n",
    "print('La media de la posicion es: {}'.format(positions[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088be36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize = (9,5))\n",
    "plt.title('Grafico caja y bigotes')\n",
    "sns.boxplot(position_letters_dict[positions[8]])\n",
    "\n",
    "plt.ylabel('Repeticiones')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_words = []\n",
    "for word in words_final_list:\n",
    "    cont = 0\n",
    "    for word2 in words_filter_list:\n",
    "        if word == word2:\n",
    "            cont += 1\n",
    "    frequency_words.append(cont)\n",
    "\n",
    "#print(frequency_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# Gráfico Q-Q\n",
    "# ==============================================================================\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "sm.qqplot(\n",
    "    peso,\n",
    "    fit   = True,\n",
    "    line  = 'q',\n",
    "    alpha = 0.4,\n",
    "    lw    = 2,\n",
    "    ax    = ax\n",
    ")\n",
    "ax.set_title('', fontsize = 10,\n",
    "             fontweight = \"bold\")\n",
    "ax.tick_params(labelsize = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a879dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words_frecuency = pd.DataFrame(list(zip(alphabet,list_letter_repetitions)),\n",
    "               columns =['words', 'frecuency'])\n",
    "\n",
    "#df_words_frecuency.sort_values(by=['frecuency'] ,ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "# make barplot and sort bars in descending order\n",
    "def addlabels(x,y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i,y[i],y[i])\n",
    "splot = sns.barplot(x='words', \n",
    "            y=\"frecuency\", \n",
    "            data=df_words_frecuency, \n",
    "            order=df_words_frecuency.sort_values('frecuency',ascending = False).words,\n",
    "            color=(0.2, 0.3, 0.6, 0.6))\n",
    "# set labels\n",
    "plt.xlabel(\"Education\", size=15)\n",
    "plt.ylabel(\"Salary in US Dollars\", size=15)\n",
    "plt.title(\"Sort Bars in Barplot in Descending Order\", size=18)\n",
    "\n",
    "\n",
    "plt.bar_label(splot.containers[0])\n",
    "plt.show()\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"sort_bars_in_barplot_descending_order_Seaborn_Python.png\", dpi=100)\n",
    "\n",
    "#ax = df_words_frecuency.plot.bar(x='words', y='frecuency', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5305a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words_frecuency = pd.DataFrame(list(zip(alphabet,list_letter_repetitions)),\n",
    "               columns =['words', 'frecuency'])\n",
    "\n",
    "#df_words_frecuency.sort_values(by=['frecuency'] ,ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "# make barplot and sort bars in descending order\n",
    "def addlabels(x,y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i,y[i],y[i])\n",
    "splot = sns.barplot(x='words', \n",
    "            y=\"frecuency\", \n",
    "            data=df_words_frecuency, \n",
    "            order=df_words_frecuency.sort_values('frecuency',ascending = False).words,\n",
    "            color=(0.2, 0.3, 0.6, 0.6))\n",
    "# set labels\n",
    "plt.xlabel(\"Education\", size=15)\n",
    "plt.ylabel(\"Salary in US Dollars\", size=15)\n",
    "plt.title(\"Sort Bars in Barplot in Descending Order\", size=18)\n",
    "\n",
    "\n",
    "plt.bar_label(splot.containers[0])\n",
    "plt.show()\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"sort_bars_in_barplot_descending_order_Seaborn_Python.png\", dpi=100)\n",
    "\n",
    "#ax = df_words_frecuency.plot.bar(x='words', y='frecuency', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# Histograma + curva normal teórica\n",
    "# ==============================================================================\n",
    "peso = df_words_frecuency['frecuency']\n",
    "# Valores de la media (mu) y desviación típica (sigma) de los datos\n",
    "mu, sigma = stats.norm.fit(peso)\n",
    "\n",
    "# Valores teóricos de la normal en el rango observado\n",
    "x_hat = np.linspace(min(peso), max(peso), num=100)\n",
    "y_hat = stats.norm.pdf(x_hat, mu, sigma)\n",
    "\n",
    "# Gráfico\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax.plot(x_hat, y_hat, linewidth=2, label='normal')\n",
    "ax.hist(x=peso, density=True, bins=30, color=\"#3182bd\", alpha=0.5)\n",
    "ax.plot(peso, np.full_like(peso, -0.01), '|k', markeredgewidth=1)\n",
    "ax.set_title('')\n",
    "ax.set_xlabel('peso')\n",
    "ax.set_ylabel('Densidad de probabilidad')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_test = {'a':{'position_distribution':[1,2,3,6,8],\n",
    "                  'total_amount':5},\n",
    "             'b':{'position_distribution':[1,2,3,6,9],\n",
    "                  'total_amount':5}}\n",
    "dummy_test\n",
    "#positions= list_dict_data(['a']['position_distribution'])\n",
    "#positions\n",
    "lista = []\n",
    "\n",
    "\n",
    "print(list(dummy_test['a']['position_distribution']))\n",
    "#a = [j for j in ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words_frecuency = pd.DataFrame(list(zip(xaxis[4:],words_final_len[0][4:])),\n",
    "               columns =['words', 'frecuency'])\n",
    "\n",
    "df_words_frecuency.sort_values(by=['frecuency'] ,ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "# make barplot and sort bars in descending order\n",
    "def addlabels(x,y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i,y[i],y[i])\n",
    "sns.barplot(x='words', \n",
    "            y=\"frecuency\", \n",
    "            data=df_words_frecuency, \n",
    "            order=df_words_frecuency.sort_values('frecuency',ascending = False).words,\n",
    "            color=(0.2, 0.3, 0.6, 0.6))\n",
    "# set labels\n",
    "plt.xlabel(\"Longitud de las palabras\", size=15)\n",
    "plt.ylabel(\"Numero de palabras\", size=15)\n",
    "plt.title(\"Sort Bars in Barplot in Descending Order\", size=18)\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"sort_bars_in_barplot_descending_order_Seaborn_Python.png\", dpi=100)\n",
    "\n",
    "#ax = df_words_frecuency.plot.bar(x='words', y='frecuency', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08173387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32430\n",
      "32430\n",
      "[]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open(\"Data_Base_Final_Words.txt\", 'r') as raw_file:\n",
    "\n",
    "  logs_text = raw_file.read()\n",
    "\n",
    "#print(logs_text)\n",
    "words_filter_list = re.sub(\"[\\s]\", \" \",logs_text).split()\n",
    "print(len(words_filter_list))\n",
    "\n",
    "\n",
    "with open('final_cleanLauraV4.txt', 'r',encoding ='utf-8') as raw_file:\n",
    "\n",
    "  logs_text2 = raw_file.read()\n",
    "\n",
    "words_filter_list2 = re.sub(\"[\\s]\", \" \",logs_text2).split()\n",
    "print(len(words_filter_list2))\n",
    "nuevalista=[]\n",
    "for i in words_filter_list:\n",
    "    if i not in words_filter_list2:\n",
    "        nuevalista.append(i)\n",
    "print(nuevalista)\n",
    "print('mata' in words_filter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911df33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print(\"\\n-------------------------------------------------------------------\\n\")\n",
    "stop_words = stopwords.words('spanish')\n",
    "\n",
    "print('habia' in stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
