{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6f693b",
   "metadata": {},
   "source": [
    "# Notebook para la creacion de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c4f1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of text files names\n",
    "txt_files = [\n",
    "    '_C3_81frica.txt','Aeropuerto.txt','Agricultura.txt','Alan_Turing.txt',\n",
    "    'Antiguo_Egipto.txt','Arte.txt','Asia.txt','Babilonia_(ciudad).txt',\n",
    "    'Biblia.txt','Biodiversidad.txt','Capitalismo.txt','Ciencia.txt','Colombia.txt',\n",
    "    'Consumo.txt','Cristianismo.txt','Cristo.txt','Cultura.txt','Democracia.txt',\n",
    "    'Diccionario.txt','Divina_comedia.txt','Ecologismo.txt','Energ_C3_ADa_nuclear.txt',\n",
    "    'Enrique_VIII_de_Inglaterra.txt','Escritura.txt','Est_C3_A9tica.txt','Estado.txt',\n",
    "    'Estados_Unidos.txt','Europa.txt','Filosof_C3_ADa.txt','Finanzas.txt','Fruta.txt',\n",
    "    'Gabriel_Garc_C3_ADa_M_C3_A1rquez.txt','Gastronom_C3_ADa_de_M_C3_A9xico.txt','George_Patton.txt',\n",
    "    'Grecia.txt','Guerra_Fr_C3_ADa.txt','Henry_Ford.txt','Holocausto.txt','Homo_sapiens.txt',\n",
    "    'Inteligencia_artificial.txt','Italia.txt','Jorge_Isaacs.txt','Juda_C3_ADsmo.txt','Juglar.txt',\n",
    "    'Julio_Garavito.txt','Liberalismo.txt','Libro.txt','Ludwig_van_Beethoven.txt','Nevado_del_Ruiz.txt',\n",
    "    'Nikola_Tesla.txt','Odisea.txt','Organizaci_C3_B3n_de_las_Naciones_Unidas.txt',\n",
    "    'Per_C3_ADodo_helen_C3_ADstico.txt','Pizza.txt','Pueblos_germ_C3_A1nicos.txt','Real_Academia_Espa_C3_B1ola.txt',\n",
    "    'Reino_Unido.txt','Revoluci_C3_B3n_Industrial.txt','Rueda.txt','Salsa_(g_C3_A9nero_musical).txt',\n",
    "    'Saxof_C3_B3n.txt','Segunda_Guerra_Mundial.txt','Sublime.txt','Thomas_Alva_Edison.txt','Valle.txt'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e32d215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436289\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "from unicodedata import normalize \n",
    "for txt in txt_files:\n",
    "    with open(txt, 'r',encoding='utf-8') as raw_file:\n",
    "\n",
    "      logs_text = raw_file.read()\n",
    "    \n",
    "    words_filter_list = re.sub(\"[\\s]\", \" \",logs_text).split()\n",
    "    ##\n",
    "with open(\"Data_Base_complete.txt\", 'r',encoding='utf-8') as raw_file:\n",
    "\n",
    "  logs_text = raw_file.read()\n",
    "\n",
    "list_of_words = re.sub(\"[\\s]\", \" \",logs_text).split()\n",
    "print(len(list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Program\n",
    "import re \n",
    "from unicodedata import normalize \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## for loop to open and filter the words of every file on the list\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('spanish')\n",
    "for txt in txt_files:\n",
    "    with open(txt, 'r',encoding='utf-8') as raw_file:\n",
    "\n",
    "      logs_text = raw_file.read()\n",
    "    \n",
    "    # -> NFD y eliminar diacríticos\n",
    "    remove_diacriticals = re.sub(r\"([^\\u0301])[\\u0301]\",r\"\\1\", \n",
    "            normalize( \"NFD\", logs_text)\n",
    "            )\n",
    "    remove_diacriticals = normalize( 'NFC', remove_diacriticals)\n",
    "    ##\n",
    "    remove_symbols = re.sub(r'[^\\w^n\\u0303|\\s]|_', ' ', remove_diacriticals)\n",
    "    ##\n",
    "    only_letters = re.sub(r\"\\S*[^a-zñA-ZÑ\\s]\\S*\", \"\", remove_symbols)\n",
    "    ##\n",
    "    def doble_upper(match_obj):\n",
    "        char_elem = match_obj.group(0)\n",
    "        count = 0\n",
    "        for letter in char_elem:\n",
    "            if letter.isupper():\n",
    "                count +=1\n",
    "            if count >= 2:\n",
    "                return \"\"\n",
    "                break\n",
    "        return char_elem\n",
    "    remove_acronyms = re.sub(r\"\\S*\\w\\S*\", doble_upper, only_letters)\n",
    "    ##----------------\n",
    "    def reverse_case(match_obj):\n",
    "        char_elem = match_obj.group(0)\n",
    "        return char_elem.lower()\n",
    "\n",
    "    remove_uppercase = re.sub(r\"([A-ZÑ])\",reverse_case, remove_acronyms)\n",
    "    list_of_words = re.sub(\"[^\\w|^n\\u0303]\", \" \",remove_uppercase).split()\n",
    "    list_large_words=[]\n",
    "    for word in list_of_words:\n",
    "        if len(word)>3:\n",
    "            list_large_words.append(word)\n",
    "    \n",
    "    list_filter_words = []\n",
    "\n",
    "    for word in list_large_words:\n",
    "        if word not in stop_words:\n",
    "            list_filter_words.append(word)\n",
    "    ##\n",
    "    data_base_filter = open(\"Data_Base_Words_Filter.txt\", \"a\")\n",
    "\n",
    "\n",
    "    list_final_words = []\n",
    "    for word in list_filter_words:\n",
    "        if word not in list_final_words:\n",
    "            list_final_words.append(word)\n",
    "    ##\n",
    "    for word in list_filter_words:\n",
    "        data_base_filter.write(word)\n",
    "        data_base_filter.write(' ')\n",
    "    data_base_filter.close()  \n",
    "    ##\n",
    "\n",
    "with open(\"Data_Base_Words_Filter.txt\", 'r') as raw_file:\n",
    "\n",
    "  logs_text = raw_file.read()\n",
    "data_base_final = open(\"Data_Base_Final_Words.txt\", \"a\")\n",
    "#print(logs_text)\n",
    "list_of_words = re.sub(\"[\\s]\", \" \",logs_text).split()\n",
    "print(len(list_of_words))\n",
    "final_list_of_words = []\n",
    "for word in list_of_words:\n",
    "    if word not in final_list_of_words:\n",
    "        final_list_of_words.append(word)\n",
    "for word in final_list_of_words:\n",
    "    data_base_final.write(word)\n",
    "    data_base_final.write(' ')\n",
    "data_base_final.close()       \n",
    "print('estados' in list_filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed934e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a16554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
