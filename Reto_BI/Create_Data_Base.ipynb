{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6f693b",
   "metadata": {},
   "source": [
    "# Notebook para la creacion de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff50e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from unicodedata import normalize \n",
    "import os, glob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e32d215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430020\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/JuanJBedoya/Training_Analitica/data_understanding/Reto_BI/Data'\n",
    "file_test = open('Data_Base_complete.txt','w')\n",
    "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
    "  with open(os.path.join(os.getcwd(), filename), 'r',encoding='utf-8') as raw_file:\n",
    "\n",
    "      logs_text = raw_file.read()\n",
    "    \n",
    "      words_filter_list = re.sub(\"[\\s]\", \" \",logs_text).split()\n",
    "    ##\n",
    "  for word in words_filter_list:\n",
    "        file_test.write(word)\n",
    "        file_test.write(' ')    \n",
    "\n",
    "file_test.close()\n",
    "with open(\"Data_Base_complete.txt\", 'r',encoding='utf-8') as raw_file:\n",
    "\n",
    "  logs_text = raw_file.read()\n",
    "\n",
    "list_of_words = re.sub(\"[\\s]\", \" \",logs_text).split()\n",
    "print(len(list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3581b59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/JuanJBedoya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214598\n",
      "32430\n"
     ]
    }
   ],
   "source": [
    "#Mean Program\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('spanish')\n",
    "data_base_filter = open(\"Data_Base_Words_Filter.txt\", \"w\",encoding='utf-8')\n",
    "data_base_filter.close() \n",
    "path = '/Users/JuanJBedoya/Training_Analitica/data_understanding/Reto_BI/Data'\n",
    "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
    "   with open(os.path.join(os.getcwd(), filename), 'r',encoding='utf-8') as raw_file:\n",
    "\n",
    "    logs_text = raw_file.read()\n",
    "    \n",
    "    def normalize(s):\n",
    "        replacements = (\n",
    "            (\"á\", \"a\"),\n",
    "            (\"é\", \"e\"),\n",
    "            (\"í\", \"i\"),\n",
    "            (\"ó\", \"o\"),\n",
    "            (\"ú\", \"u\"),\n",
    "        )\n",
    "\n",
    "        for a, b in replacements:\n",
    "            s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "        return s\n",
    "\n",
    "    remove_diacriticals = normalize(logs_text)\n",
    "    ##\n",
    "    remove_symbols = re.sub(r'[^\\w^n\\u0303|\\s]|_', ' ', remove_diacriticals)\n",
    "    ##\n",
    "    def doble_upper(match_obj):\n",
    "        char_elem = match_obj.group(0)\n",
    "        count = 0\n",
    "        for letter in char_elem:\n",
    "            if letter.isupper():\n",
    "                count +=1\n",
    "            if count >= 2:\n",
    "                return \"\"\n",
    "                break\n",
    "        return char_elem\n",
    "    remove_acronyms = re.sub(r\"\\S*\\w\\S*\", doble_upper, remove_symbols)\n",
    "        ##\n",
    "    only_letters = re.sub(r\"[^a-zñA-ZÑ]\", \" \", remove_acronyms)\n",
    "    ##----------------\n",
    "    def reverse_case(match_obj):\n",
    "        char_elem = match_obj.group(0)\n",
    "        return char_elem.lower()\n",
    "\n",
    "    remove_uppercase = re.sub(r\"([A-ZÑ])\",reverse_case, only_letters)\n",
    "    list_of_words = re.sub(\"[^\\w|^n\\u0303]\", \" \",remove_uppercase).split()\n",
    "    list_large_words=[]\n",
    "    for word in list_of_words:\n",
    "        if len(word)>3:\n",
    "            list_large_words.append(word)\n",
    "    \n",
    "    list_filter_words = []\n",
    "\n",
    "    for word in list_large_words:\n",
    "        if word not in stop_words:\n",
    "            list_filter_words.append(word)\n",
    "    ##\n",
    "    data_base_filter = open(\"Data_Base_Words_Filter.txt\", \"a\",encoding='utf-8')\n",
    "\n",
    "\n",
    "    list_final_words = []\n",
    "    for word in list_filter_words:\n",
    "        if word not in list_final_words:\n",
    "            list_final_words.append(word)\n",
    "    ##\n",
    "    for word in list_filter_words:\n",
    "        data_base_filter.write(word)\n",
    "        data_base_filter.write(' ')\n",
    "    data_base_filter.close()  \n",
    "    ##\n",
    "\n",
    "with open(\"Data_Base_Words_Filter.txt\", 'r',encoding='utf-8') as raw_file:\n",
    "\n",
    "  logs_text = raw_file.read()\n",
    "data_base_final = open(\"Data_Base_Final_Words.txt\", \"w\",encoding='utf-8')\n",
    "\n",
    "list_of_words = re.sub(\"[\\s]\", \" \",logs_text).split()\n",
    "print(len(list_of_words))\n",
    "final_list_of_words = []\n",
    "for word in list_of_words:\n",
    "    if word not in final_list_of_words:\n",
    "        final_list_of_words.append(word)\n",
    "for word in final_list_of_words:\n",
    "    data_base_final.write(word)\n",
    "    data_base_final.write(' ')\n",
    "data_base_final.close()       \n",
    "\n",
    "print(len(final_list_of_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 ('venv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "e16987386912a2ab5ce2b083eb4c6af33b693eb6294167d13947179556e43b0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
