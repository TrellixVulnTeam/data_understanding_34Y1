{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6f693b",
   "metadata": {},
   "source": [
    "# Notebook para la creacion de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ff50e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\James\n",
      "[nltk_data]     Bedoya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "from unicodedata import normalize \n",
    "import os, glob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords \n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5cfeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change vowels with accent to the same without it\n",
    "\n",
    "def remove_diacriticals(words):\n",
    "    replacements = (\n",
    "        ('á', 'a'),\n",
    "        ('é', 'e'),\n",
    "        ('í', 'i'),\n",
    "        ('ó', 'o'),\n",
    "        ('ú', 'u'),\n",
    "    )\n",
    "\n",
    "    for a, b in replacements:\n",
    "        words = words.replace(a, b).replace(a.upper(), b.upper())\n",
    "    return words\n",
    "\n",
    "# Replace symbols and _ for space\n",
    "\n",
    "def remove_symbols(words):\n",
    "    remove = re.sub(r'[^\\w^n\\u0303|\\s]|_', ' ', words)\n",
    "    return remove\n",
    "\n",
    "# Remove words with at least two uppercase letters \n",
    "\n",
    "def remove_acronyms(words):\n",
    " \n",
    "    def doble_upper(match_obj):\n",
    "        char_elem = match_obj.group(0)\n",
    "        count = len(re.findall(r'[A-Z]',char_elem))\n",
    "        if count >= 2:\n",
    "            return ''\n",
    "        else:\n",
    "            return char_elem\n",
    "    \n",
    "    remove = re.sub(r'\\S*\\w\\S*', doble_upper, words)\n",
    "    return remove\n",
    "\n",
    "# Change numbers for space\n",
    "\n",
    "def only_letters(words): \n",
    "    remove = re.sub(r'[^a-zñA-ZÑ]', ' ', words)\n",
    "    return remove \n",
    "\n",
    "# Change uppercase to lowercase\n",
    "\n",
    "def remove_uppercase(words):\n",
    "        \n",
    "    def reverse_case(match_obj):\n",
    "        char_elem = match_obj.group(0)\n",
    "        return char_elem.lower()\n",
    "    remove = re.sub(r'([A-ZÑ])',reverse_case, words)\n",
    "    return remove\n",
    "\n",
    "# Remove words with less than 4 letters long\n",
    "\n",
    "def remove_short_word(words):\n",
    "    list_of_words = words.split(' ')\n",
    "    list_large_words=[]\n",
    "    for word in list_of_words:\n",
    "        if len(word)>3:\n",
    "            list_large_words.append(word)\n",
    "    return list_large_words\n",
    "\n",
    "# Remove stopswords in library nltk\n",
    "\n",
    "def remove_stopwords(words_list):\n",
    "    stop_words = stopwords.words('spanish')\n",
    "    list_filter_words = []\n",
    "    for word in words_list:\n",
    "        if word not in stop_words:\n",
    "            list_filter_words.append(word)\n",
    "    return list_filter_words\n",
    "\n",
    "# All the steps to clean a text\n",
    "\n",
    "def clean_words(text):\n",
    "    clean_dia = remove_diacriticals(text)\n",
    "    clean_sym = remove_symbols(clean_dia)\n",
    "    clean_acro = remove_acronyms(clean_sym)\n",
    "    clean_lett = only_letters(clean_acro)\n",
    "    clean_upp = remove_uppercase(clean_lett)\n",
    "    clean_shor = remove_short_word(clean_upp)\n",
    "    clean_text = remove_stopwords(clean_shor)\n",
    "    return clean_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beb0df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open all files\n",
    "# Files location path\n",
    "#path = '/Users/JuanJBedoya/Training_Analitica/data_understanding/Reto_BI/Data'\n",
    "path = '/Users/James Bedoya/proyecto1/Reto_BI/Data'\n",
    "# create list of all files from path \n",
    "text_files_list = []\n",
    "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
    "   with open(os.path.join(path, filename), 'r',encoding='utf-8') as raw_file:\n",
    "\n",
    "    logs_text = raw_file.read()\n",
    "    text_files_list.append(logs_text)\n",
    "\n",
    "words_text = ' '.join(text_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3581b59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214598\n",
      "32430\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clean_all_text = clean_words(words_text)\n",
    "\n",
    "# Open file and write all the words clean\n",
    "\n",
    "with open('Data_Base_Words_Filter.txt', 'w',encoding='utf-8') as filter_file:\n",
    "\n",
    "  filter_file.write('\\n'.join(clean_all_text)) \n",
    "\n",
    "#  Open file and write all the words without repeats\n",
    "\n",
    "final_list_of_words = list(set(clean_all_text))\n",
    "with open('Data_Base_Final_Words.txt', 'w',encoding='utf-8') as final_file:\n",
    "\n",
    "  final_file.write('\\n'.join(final_list_of_words))\n",
    "print(len(clean_all_text))\n",
    "print(len(final_list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16b4b839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430020\n"
     ]
    }
   ],
   "source": [
    "# Code to know how many words the originals files have\n",
    "\n",
    "path = '/Users/JuanJBedoya/Training_Analitica/data_understanding/Reto_BI/Data'\n",
    "file_test = open('Data_Base_complete.txt','w')\n",
    "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
    "  with open(os.path.join(os.getcwd(), filename), 'r',encoding='utf-8') as raw_file:\n",
    "\n",
    "      logs_text = raw_file.read()\n",
    "    \n",
    "      words_filter_list = re.sub('[\\s]', ' ',logs_text).split()\n",
    "    ##\n",
    "  for word in words_filter_list:\n",
    "        file_test.write(word)\n",
    "        file_test.write(' ')    \n",
    "\n",
    "file_test.close()\n",
    "with open('Data_Base_complete.txt', 'r',encoding='utf-8') as raw_file:\n",
    "\n",
    "  logs_text = raw_file.read()\n",
    "\n",
    "list_of_words = re.sub('[\\s]', ' ',logs_text).split()\n",
    "print(len(list_of_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 ('venv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "e16987386912a2ab5ce2b083eb4c6af33b693eb6294167d13947179556e43b0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
